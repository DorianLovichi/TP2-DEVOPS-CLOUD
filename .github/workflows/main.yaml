name: CI/CD Pipeline for EKS

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: campaign-manager
  EKS_CLUSTER_NAME: campaign-manager-cluster
  KUBERNETES_NAMESPACE: campaign-manager

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.13.3"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest
          if [ -f backend/requirements.txt ]; then pip install -r backend/requirements.txt; fi
          if [ -f backend/test-requirements.txt ]; then pip install -r backend/test-requirements.txt; fi
          pip install moto pytest-flask flake8

      - name: Lint with flake8
        run: |
          flake8 backend --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 backend --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Test with pytest
        run: pytest

  build-and-push:
    needs: test
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Create ECR repository if it doesn't exist
        run: |
          if ! aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "Creating ECR repository ${{ env.ECR_REPOSITORY }}..."
            aws ecr create-repository --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }}
          else
            echo "ECR repository ${{ env.ECR_REPOSITORY }} already exists."
          fi

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build, tag, and push image to Amazon ECR
        id: build-image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Build a docker container and push it to ECR
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT

  deploy:
    needs: build-and-push
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install AWS CLI and Kubernetes tools
        run: |
          # Installer kubectl
          curl -LO "https://dl.k8s.io/release/v1.29.0/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

          # Installer AWS CLI
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install --update

          # Installer eksctl
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Debug - Check caller identity
        run: aws sts get-caller-identity

      - name: Debug - Check EKS clusters
        run: aws eks list-clusters --region ${{ env.AWS_REGION }}

      - name: Check if EKS cluster exists and get auth token
        id: check-eks
        run: |
          # Vérifier si le cluster existe
          if aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "Cluster ${{ env.EKS_CLUSTER_NAME }} exists, continuing with deployment."
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
            
            # Créer/mettre à jour la configuration kubectl
            aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
            
            # Vérifier la configuration kubectl
            kubectl config view --minify
            
            # Obtenir un jeton d'authentification pour le cluster
            TOKEN=$(aws eks get-token --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} | jq -r '.status.token')
            echo "eks_token=$TOKEN" >> $GITHUB_OUTPUT
          else
            echo "Warning: Cluster ${{ env.EKS_CLUSTER_NAME }} does not exist or you don't have permission to access it."
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: EKS Access Configuration
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "Configuring access for IAM user"

          # Obtenir l'ARN de l'utilisateur IAM actuel
          USER_ARN=$(aws sts get-caller-identity --query 'Arn' --output text)
          USER_NAME=$(echo $USER_ARN | cut -d '/' -f 2)
          echo "Current user ARN: $USER_ARN"

          # Vérifier si les accès EKS sont activés
          if aws eks list-access-entries --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "Using EKS Access Entries"
            
            # Créer une entrée d'accès pour l'utilisateur actuel
            aws eks create-access-entry \
              --cluster-name ${{ env.EKS_CLUSTER_NAME }} \
              --principal-arn $USER_ARN \
              --region ${{ env.AWS_REGION }} || echo "Access entry may already exist"
            
            # Associer la politique d'administration
            aws eks associate-access-policy \
              --cluster-name ${{ env.EKS_CLUSTER_NAME }} \
              --principal-arn $USER_ARN \
              --policy-arn "arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy" \
              --access-scope '{"type": "cluster"}' \
              --region ${{ env.AWS_REGION }} || echo "Policy may already be associated"
          else
            echo "Using ConfigMap for authentication"
            
            # Créer un ConfigMap temporaire pour l'authentification
            cat > aws-auth-configmap.yaml << EOF
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: aws-auth
              namespace: kube-system
            data:
              mapUsers: |
                - userarn: $USER_ARN
                  username: $USER_NAME
                  groups:
                  - system:masters
            EOF
            
            # Essayer d'appliquer le ConfigMap
            kubectl apply -f aws-auth-configmap.yaml --validate=false || echo "Could not apply ConfigMap directly"
            
            # Si l'application directe échoue, essayer de fusionner avec un ConfigMap existant
            kubectl get configmap aws-auth -n kube-system -o yaml || echo "ConfigMap not found"
          fi

          # Mise à jour de la configuration kubectl
          aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

          # Vérification finale de l'accès
          kubectl auth can-i '*' '*' || echo "Could not verify permissions"

      - name: Deploy to EKS
        if: steps.check-eks.outputs.cluster_exists == 'true'
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Create namespace if doesn't exist
          kubectl create namespace ${{ env.KUBERNETES_NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f - --validate=false

          # Create Kubernetes deployment YAML dynamically
          cat > k8s-deployment.yaml << EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: campaign-manager
            labels:
              app: campaign-manager
          spec:
            replicas: 3
            selector:
              matchLabels:
                app: campaign-manager
            template:
              metadata:
                labels:
                  app: campaign-manager
              spec:
                containers:
                - name: campaign-manager
                  image: ${ECR_REGISTRY}/${ECR_REPOSITORY}:${IMAGE_TAG}
                  ports:
                  - containerPort: 5000
                  env:
                  - name: DYNAMODB_TABLE
                    value: "Campaigns"
                  - name: AWS_REGION
                    value: "${{ env.AWS_REGION }}"
          EOF

          # Create Kubernetes service YAML dynamically
          cat > k8s-service.yaml << EOF
          apiVersion: v1
          kind: Service
          metadata:
            name: campaign-manager
            labels:
              app: campaign-manager
          spec:
            type: LoadBalancer
            ports:
            - port: 80
              targetPort: 5000
              protocol: TCP
            selector:
              app: campaign-manager
          EOF

          # Apply Kubernetes manifests with validation disabled
          kubectl apply -f k8s-deployment.yaml -n ${{ env.KUBERNETES_NAMESPACE }} --validate=false
          kubectl apply -f k8s-service.yaml -n ${{ env.KUBERNETES_NAMESPACE }} --validate=false

          # Wait for deployment to be ready
          kubectl rollout status deployment/campaign-manager -n ${{ env.KUBERNETES_NAMESPACE }} --timeout=300s || true

          # Get service URL
          echo "Service URL:"
          kubectl get svc campaign-manager -n ${{ env.KUBERNETES_NAMESPACE }}
